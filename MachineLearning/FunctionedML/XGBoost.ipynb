{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16690 files belonging to 2 classes.\n",
      "Using 13352 files for training.\n",
      "Found 16690 files belonging to 2 classes.\n",
      "Using 3338 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 100\n",
    "seed = 123\n",
    "dir=\"C:/Users/charl/Desktop/Final year Project/GitHubApi/training/functionedDatabase\"\n",
    "train_ds=tf.keras.utils.text_dataset_from_directory(dir, labels='inferred', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds=tf.keras.utils.text_dataset_from_directory(dir, labels='inferred', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "class_names = sorted(os.listdir(dir))\n",
    "class_dict = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "train_labels=train_ds.map(lambda x, y: \"pos\" if y == 1 else \"neg\")\n",
    "val_labels = val_ds.map(lambda x, y: \"pos\" if y == 1 else \"neg\")\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)\n",
    "\n",
    "\n",
    "\n",
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()\n",
    "\n",
    "result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))\n",
    "result.shape\n",
    "\n",
    "\n",
    "\n",
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence. ####changed the vocab size from 10,000 50,000\n",
    "vocab_size = 50000\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "\n",
    "embedding_dim=50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'text_vectorization_5' (type TextVectorization).\n\nAttempt to convert a value (<MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>) to a Tensor.\n\nCall arguments received by layer 'text_vectorization_5' (type TextVectorization):\n  • inputs=<MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m val_data \u001b[39m=\u001b[39m val_ds\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (vectorize_layer(x), y))\u001b[39m.\u001b[39mprefetch(buffer_size\u001b[39m=\u001b[39mAUTOTUNE)\n\u001b[0;32m      5\u001b[0m val_text_ds \u001b[39m=\u001b[39m val_ds\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: x)\n\u001b[1;32m----> 6\u001b[0m val_features \u001b[39m=\u001b[39m vectorize_layer(val_text_ds)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      9\u001b[0m embedding_layer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mEmbedding(vocab_size, embedding_dim)\n\u001b[0;32m     10\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m     11\u001b[0m     embedding_layer,\n\u001b[0;32m     12\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mGlobalAveragePooling1D(),\n\u001b[0;32m     13\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m ])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'text_vectorization_5' (type TextVectorization).\n\nAttempt to convert a value (<MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>) to a Tensor.\n\nCall arguments received by layer 'text_vectorization_5' (type TextVectorization):\n  • inputs=<MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the text data into a feature matrix using the embedding and vectorize layers\n",
    "train_data = train_ds.map(lambda x, y: (vectorize_layer(x), y)).prefetch(buffer_size=AUTOTUNE)\n",
    "val_data = val_ds.map(lambda x, y: (vectorize_layer(x), y)).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_text_ds = val_ds.map(lambda x, y: x)\n",
    "val_features = vectorize_layer(val_text_ds).numpy()\n",
    "\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "model = tf.keras.Sequential([\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model for a few epochs\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# weights=weights.astype('float32')\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# vectorize_layer=vectorize_layer.astype('float32')\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m vectorize_layer\u001b[39m.\u001b[39;49mset_weights([weights])\n\u001b[0;32m     10\u001b[0m text_data \u001b[39m=\u001b[39m text_ds\u001b[39m.\u001b[39mbatch(batch_size)\u001b[39m.\u001b[39mprefetch(AUTOTUNE)\n\u001b[0;32m     11\u001b[0m features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(model\u001b[39m.\u001b[39mpredict(text_data))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\base_layer.py:1802\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1800\u001b[0m     num_tensors \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39mnum_tensors\n\u001b[0;32m   1801\u001b[0m     tensors \u001b[39m=\u001b[39m weights[weight_index : weight_index \u001b[39m+\u001b[39m num_tensors]\n\u001b[1;32m-> 1802\u001b[0m     param\u001b[39m.\u001b[39;49mset_weights(tensors)\n\u001b[0;32m   1803\u001b[0m     weight_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_tensors\n\u001b[0;32m   1804\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py:89\u001b[0m, in \u001b[0;36mVocabWeightHandler.set_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_weights\u001b[39m(\u001b[39mself\u001b[39m, weights):\n\u001b[1;32m---> 89\u001b[0m     tokens \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(weights[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m     90\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layer\u001b[39m.\u001b[39mlookup_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layer\u001b[39m.\u001b[39m_lookup_table_from_tokens(tokens)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "#print(\"testing\")\n",
    "# Extract the embedding weights from the model and use them to transform the text data into a feature matrix\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "weights=np.asarray(weights).astype('float32')\n",
    "print(\"testing\")\n",
    "\n",
    "# weights=weights.astype('float32')\n",
    "# vectorize_layer=vectorize_layer.astype('float32')\n",
    "#vectorize_layer.set_weights([weights])\n",
    "text_data = text_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "features = np.array(model.predict(text_data))\n",
    "\n",
    "# # Train an XGBoost model on the feature matrix\n",
    "dtrain = xgb.DMatrix(features, label=train_labels)\n",
    "dval = xgb.DMatrix(val_features, label=val_labels)\n",
    "params = {'objective': 'binary:logistic', 'eval_metric': 'logloss'}\n",
    "model = xgb.train(params, dtrain, evals=[(dval, 'validation')], num_boost_round=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a6b0d4c64939c7b9221b8ea16fca0b57a3698d780dfef28f0ba575c486249f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
